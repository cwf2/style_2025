{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa60504f-7cc9-4fd3-aa46-9127cb6de7f1",
   "metadata": {},
   "source": [
    "# Working with text\n",
    "\n",
    "In the second part of the workshop, we go beyond the speech metadata recorded by DICES. We're going to download the text of the speeches from the Perseus Digital Library and then parse the Latin and Greek with the natural language processing framework spaCy.\n",
    "\n",
    "The DICES client includes basic functionality for working with speech text, including convenience functions that wrap third-party tools for interacting with Perseus and spaCy. If you want to do more complex tasks, or work with Greek and Latin passages that aren't speeches, you can use these tools directly.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Install DICES client software\n",
    "\n",
    "Because Google Colab runs this notebook on a fresh virtual machine every time, we always need to install DICES as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849d166-8505-451a-bdee-e8b2bf50121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert pip for language model compatibility\n",
    "!pip install -q --force-reinstall pip==22.0.0\n",
    "# install DICES client\n",
    "!pip install -q git+https://github.com/cwf2/dices-client\n",
    "# install language models\n",
    "!pip install -q https://huggingface.co/latincy/la_core_web_md/resolve/main/la_core_web_md-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2ce39-1a82-4145-8e56-dfe49c2e5cc8",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Here we tell Python which ancillary packages we want to make accessible. In this case, the DICES client and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf88b6-9143-4b07-a02c-c48f6d575840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for talking to DICES\n",
    "from dicesapi import DicesAPI\n",
    "# for talking to Perseus\n",
    "from dicesapi.text import CtsAPI\n",
    "\n",
    "# for displaying HTML\n",
    "from IPython.display import HTML\n",
    "# for creating data tables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f54a8-14f7-42eb-b10c-f0ebe62e4824",
   "metadata": {},
   "source": [
    "### Connect to external resources\n",
    "Here we instantiate a connection to the DICES database as in our earlier examples. We also create a second connection, this time to an external digital library. By default, `CtsAPI` uses the Perseus Digital Library's [CTS endpoint](https://scaife-cts.perseus.org/api/cts?request=GetCapabilities). \n",
    "\n",
    "Not all of the texts in Perseus are currently accessible via this interface, but its capabilities continue to improve. For now, we need to check whether a given speech has downloaded successfully before we can use its text in our scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d02c2-a115-41f8-b758-b54d6f27295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DICES connection\n",
    "api = DicesAPI(logfile=\"dices.log\", logdetail=0)\n",
    "\n",
    "# create a new CTS connection\n",
    "cts = CtsAPI(dices_api=api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2d000-9269-49df-ada1-2a180f8e1a53",
   "metadata": {},
   "source": [
    "## Download some speeches\n",
    "\n",
    "In these examples, we're going to look at a subset of speeches. Downloading and parsing the text of every speech in DICES takes about half an hour and exceeds the computational limits of Colab's free tier.\n",
    "\n",
    "For this example, we will try to compare two speeches by female characters who are bidding a male hero farewell: Calypso to Odysseus in *Odyssey* 5 and Dido to Aeneas in *Aeneid* 4.\n",
    "\n",
    "### Speech metadata from DICES\n",
    "\n",
    "The DICES client's ability to communicate very specific or complex criteria to the database server is somewhat limited. Two strategies can help:\n",
    "1. Make multiple requests. You can add together the results of multiple `getSpeeches()` calls with the `+` operator. At the moment, this can shuffle the speeches together, so you might want to sort them afterwards.\n",
    "2. Make a less specific request and then filter the results. The DICES client is better at sorting and filtering speeches once you have them in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fa700-20d6-4f43-8866-a530a113b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download metadata on speeches by women in the Odyssey and the Aeneid\n",
    "speeches = (\n",
    "    api.getSpeeches(work_title=\"Odyssey\", spkr_gender=\"female\") +\n",
    "    api.getSpeeches(work_title=\"Aeneid\", spkr_gender=\"female\")\n",
    ").sorted()\n",
    "\n",
    "# check that it worked\n",
    "print(len(speeches), \"speeches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4a792-f12e-42f5-adf1-4575ee499e57",
   "metadata": {},
   "source": [
    "### Quick overview of the results\n",
    "\n",
    "Let's just take a quick look to see what we got. It's nice to identify potentially interesting or erroneous data before we get too far into our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d05ae-b610-42b3-a7ec-069b5bcb40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of speeches\n",
    "table = pd.DataFrame(dict(\n",
    "    speech_id = speech.id,\n",
    "    author = speech.author.name,\n",
    "    work = speech.work.title,\n",
    "    locus = speech.l_range,\n",
    "    speaker = speech.getSpkrString(),\n",
    "    addressee = speech.getAddrString(),\n",
    ") for speech in speeches)\n",
    "\n",
    "# show table\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe306bd-dddc-4750-ba13-a5043663cd49",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "#### Who are the most loquatious women in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841d672-34fd-456b-9428-a6cddefe1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    table                                         # original data\n",
    "    .groupby(\"speaker\")                           # factor to group by\n",
    "    .aggregate(                                   # create a summary table\n",
    "        speeches = (\"speech_id\", \"count\"))        #   - count of speech_id\n",
    "    .sort_values(\"speeches\", ascending=False)     # sort the results from highest to lowest\n",
    "    .head(10)                                     # take the top 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee830b71-42a6-471d-92a6-fab72b7aacc3",
   "metadata": {},
   "source": [
    "#### Who do Dido and Calypso talk to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435b0e9-571a-4f97-b24f-4f4acf4ed4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    table                                         # original data\n",
    "    .query(\"speaker in ['Calypso', 'Dido']\")      # filter on speaker\n",
    "    .groupby([\"speaker\", \"addressee\"])            # group by speaker-addressee pairs\n",
    "    .aggregate(                                   # create a summary table\n",
    "        speeches = (\"speech_id\", \"count\"))        #   - count of speech_id\n",
    "    .sort_values(\"speeches\", ascending=False)     # sort from highest to lowest\n",
    "    .sort_values(\"speaker\")                       # then by speaker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252a004-3a43-468d-bf47-a90b76757bae",
   "metadata": {},
   "source": [
    "### Isolate speeches of interest\n",
    "\n",
    "The easiest way to identify the two speeches we're interested in is to look inspect individual records and note the speech IDs.\n",
    "\n",
    "Here's one way to find the Calypso speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c90c8-8b4e-400c-8b5b-93a652a792e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an anonymous view on the table of speech data\n",
    "(\n",
    "    table \n",
    "    .query(\"speaker=='Calypso'\")           # speeches by Calypso\n",
    "    .query(\"addressee=='Odysseus'\")        #   ... to Odysseus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec346a-5a63-401d-a093-73bef61252e8",
   "metadata": {},
   "source": [
    "The speech we want is the ten-line speech with ID 820.\n",
    "\n",
    "Let's look for Dido's speech next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00b0b6-0ac6-40ca-a0e7-f8cf11c10704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an anonymous view on the table of speech data\n",
    "(\n",
    "    table\n",
    "    .query(\"speaker=='Dido'\")        # speeches by Dido\n",
    "    .query(\"addressee=='Aeneas'\")    #    ... to Aeneas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02c902-6523-4c19-bc6d-b6bc938bc17a",
   "metadata": {},
   "source": [
    "The correct Dido speech has ID 1610. \n",
    "\n",
    "Knowing the IDs of the two speeches, we can create a new list of just these two speeches by filtering the larger set of results on ID. The DICES client has a collection of filtering tools: in this case, the `filterIDs()` method is what we want. It takes a list of IDs to keep and produces a new speech group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc2b4c-8b2f-43c3-bd78-beaef805b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech IDs to keep\n",
    "ids = [820, 1610]\n",
    "\n",
    "# filter the list of speeches by ID\n",
    "farewells = speeches.filterIDs(ids)\n",
    "\n",
    "# confirm that it worked\n",
    "for speech in farewells:\n",
    "    print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31a8ca-292c-4c96-8ece-90cc8525d115",
   "metadata": {},
   "source": [
    "### Download the text from Perseus\n",
    "This is the part where we download the text from the remote digital library. If you're interested, you can compare the Calypso speech in Perseus' [human interface](https://scaife.perseus.org/reader/urn:cts:greekLit:tlg0012.tlg002.perseus-grc2:5.203-5.213/) versus the [machine interface](https://scaife-cts.perseus.org/api/cts?urn=urn:cts:greekLit:tlg0012.tlg002.perseus-grc2:5.203-5.213&request=GetPassage).\n",
    "\n",
    "The `getPassage()` method takes a DICES speech as its argument. It generates an appropriate call to the machine interface and processes the XML response. In the code below, we save the results back to the speech object as a new attribute called `passage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11040c7-fdc4-4f3e-91e1-45e69959e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over two farewell speeches\n",
    "for speech in farewells:\n",
    "    # download the text from perseus\n",
    "    speech.passage = cts.getPassage(speech)\n",
    "\n",
    "    # display the results\n",
    "    print(speech, \"\\n\")\n",
    "    print(speech.passage.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cadb40-b958-4b9c-a305-54a5921f6e52",
   "metadata": {},
   "source": [
    "#### 💁🏻‍♂️ a note about DICES text passages\n",
    "\n",
    "The text returned by Perseus has line numbers embedded in it. DICES client tries to keep track of them in an attribute of the passage called `line_array`. You can see it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3386a-847b-4e7b-b587-7a8e1abcc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "farewells[0].passage.line_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83353156-f292-4db2-97b1-12d4076b6db2",
   "metadata": {},
   "source": [
    "`line_array` is a *list* of *dictionaries*. Each element of the list represents one verse line. It has two keys, `n` and `text`, whose values are the line number and verse text respectively.\n",
    "\n",
    "Meanwhile, the passage attribute called `text` joins the text together in a single string. Depending on what you want to do, you might want to iterate over the individual lines using `line_array` or work with the complete speech using `text`.\n",
    "\n",
    "*These features are relatively undeveloped so far. If you're interested in using them for something, please don't hesitate to get in touch and let me know how I can make them work more intuitively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a8980-5127-4339-990e-ae0a348c977b",
   "metadata": {},
   "source": [
    "### Parse the text using natural language processing\n",
    "\n",
    "In this step, we process the downloaded text of the speeches using a natural language processing (NLP) package for Python called [spaCy](https://spacy.io/). spaCy is a versitile NLP framework that can perform tasks including:\n",
    " - tokenization (breaking the text into words)\n",
    " - lemmatization (identifying dictionary headword for each inflected form)\n",
    " - part of speech (POS) tagging (noun, verb, adjective, etc.)\n",
    " - morphological tagging (mood, voice, case, gender, etc.)\n",
    " - dependency tagging (syntactic structure)\n",
    "\n",
    "spaCy depends on third-party language models that must be trained for each of these tasks. There are a couple of trained models for Ancient Greek and Latin, and this is currently an area of rapid change. We're going to use [ΟδύCy](https://centre-for-humanities-computing.github.io/odyCy/getting_started.html) for Greek and [LatinCy](https://huggingface.co/latincy) for Latin. Each of these projects has multiple models available, with tradeoffs between model size, processing requirements, and accuracy; we're using their \"small\" and \"medium\" models, respectively.\n",
    "\n",
    "**Note:** these models are a form of AI. Unlike older \"stemmers\" that work by a set of fixed rules (e.g. cutting off -us or -ος to find noun stems), they may produce different results for the same form depending on context. That said, they can also *make things up* just like other forms of AI.\n",
    "\n",
    "#### Running NLP\n",
    "\n",
    "The passage method `runSpacyPipeline()` runs the appropriate spaCy pipeline based on the passage's language and saves the output to a new attribute called `spacy_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd419b-787b-493d-acfb-7a7964a1bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over two farewell speeches\n",
    "for speech in farewells:\n",
    "    # run spacy\n",
    "    speech.passage.runSpacyPipeline()\n",
    "\n",
    "    # check that it worked\n",
    "    print(speech, len(speech.passage.spacy_doc), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10514b15-7b9b-4ba9-aad5-606ab79dea31",
   "metadata": {},
   "source": [
    "#### spaCy output\n",
    "\n",
    "`spacy_doc` gives you access to all the output of the NLP processing. The easiest way to use it is as a collection of *tokens*. These represent spaCy's best attempt to cut up the continuous string of text into words (or word-like elements). Each token has many attributes of its own; here are a few that might be useful:\n",
    "\n",
    "- `text`: this is the way the word looked in the original text\n",
    "- `lemma_`: this is a dictionary headword assigned by the model\n",
    "- `pos_`: this is a part of speech tag assigned by the model\n",
    "- `morph`: this is a collection of morphological tags\n",
    "\n",
    "**Notes:**\n",
    "1. Some of these attributes end in an underscore.\n",
    "2. The `morph` attribute is a collection of tags; different parts of speech may have different tags. To check for a tag, use `morph.get()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bb022-193c-4972-b2b9-8792f73a10d6",
   "metadata": {},
   "source": [
    "### Create a table of tokens\n",
    "\n",
    "This code creates a table with one row per token. Using nested `for` loops, we take each speech in turn, then each token of that speech. We build a record for the row that incorporates information about the speech and the token. At the end, we turn the list of rows into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6240a5-9cac-4c89-9d9b-84f8c6da54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty list of rows\n",
    "rows = []\n",
    "\n",
    "# iterate over speeches\n",
    "for speech in farewells:\n",
    "    \n",
    "    # iterate over tokens\n",
    "    for token in speech.passage.spacy_doc:\n",
    "\n",
    "        # create a record for this token\n",
    "        row = {\n",
    "            \"speech_id\": speech.id,\n",
    "            \"language\": speech.lang,\n",
    "            \"author\": speech.author.name,\n",
    "            \"title\": speech.work.title,\n",
    "            \"book\": speech.getPrefix(),                       # see note 1 below\n",
    "            \"line\": speech.passage.getLine(token)[\"n\"],       # see note 2\n",
    "            \"speaker\": speech.getSpkrString(),\n",
    "            \"addressee\": speech.getAddrString(),\n",
    "            \"token\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\": token.pos_,\n",
    "            \"verbform\": ''.join(token.morph.get(\"VerbForm\")), # see note 3\n",
    "            \"mood\": ''.join(token.morph.get(\"Mood\")),\n",
    "            \"voice\": ''.join(token.morph.get(\"Voice\")),\n",
    "            \"tense\": ''.join(token.morph.get(\"Tense\")),\n",
    "            \"person\": ''.join(token.morph.get(\"Person\")),\n",
    "            \"case\": ''.join(token.morph.get(\"Case\")),\n",
    "            \"gender\": ''.join(token.morph.get(\"Gender\")),\n",
    "            \"number\": ''.join(token.morph.get(\"Number\")),    \n",
    "        }\n",
    "\n",
    "        # add row to the list of rows\n",
    "        rows.append(row)\n",
    "\n",
    "# convert to a table\n",
    "tokens = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e736207-ac08-4722-b60b-1b6267cb555e",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "1. I'm using the `getPrefix()` method to get just the book number of the locus.\n",
    "2. `passage.getLine(token)` returns the one line of `line_array` that contains `token`. This allows us to give the individual line number for each token, rather than just the first and last lines for the speech as a whole.\n",
    "3. The output of `morph.get()` is always a list, but it usually has one element (if the tag exists) or zero (if it doesn't). The expression `''.join(token.morph.get(\"VerbForm\"))` checks for the tag \"VerbForm\" and joins any elements of the resulting list into a single string. If `get()` returns an empty list, the output is an empty string \"\". If `get()` returns a list of one tag, then the output is just the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023c5b9-c163-4c87-9f30-e5cca8bab445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the tokens to a CSV file\n",
    "tokens.to_csv(\"tokens.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# display the table\n",
    "display(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
