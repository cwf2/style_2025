{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa60504f-7cc9-4fd3-aa46-9127cb6de7f1",
   "metadata": {},
   "source": [
    "# Assignment: word count\n",
    "\n",
    "In this assignment we create basic word counts for two speeches: Calypso to Odysseus at *Odyssey* 5.203-5.213 and Dido to Aeneas at *Aeneid* 4.305-4.330.\n",
    "\n",
    "The first section fast-forwards through the same steps as the **Example 2** notebook, skipping some of the longer explanations. If you're continuing from that point, you can skip ahead to [Section 3](#3.-Word-counts) below.\n",
    "\n",
    "## 1. Preliminaries\n",
    "\n",
    "### Install DICES client software\n",
    "\n",
    "Because Google Colab runs this notebook on a fresh virtual machine every time, we always need to install DICES as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849d166-8505-451a-bdee-e8b2bf50121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert pip for language model compatibility\n",
    "!pip install -q --force-reinstall pip==22.0.0\n",
    "# install DICES client\n",
    "!pip install -q git+https://github.com/cwf2/dices-client\n",
    "# install language models\n",
    "!pip install -q https://huggingface.co/latincy/la_core_web_md/resolve/main/la_core_web_md-any-py3-none-any.whl\n",
    "!pip install -q https://huggingface.co/chcaa/grc_odycy_joint_sm/resolve/main/grc_odycy_joint_sm-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2ce39-1a82-4145-8e56-dfe49c2e5cc8",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Here we tell Python which ancillary packages we want to make accessible. In this case, the DICES client and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf88b6-9143-4b07-a02c-c48f6d575840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for talking to DICES\n",
    "from dicesapi import DicesAPI\n",
    "# for talking to Perseus\n",
    "from dicesapi.text import CtsAPI\n",
    "# for creating data tables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f54a8-14f7-42eb-b10c-f0ebe62e4824",
   "metadata": {},
   "source": [
    "### Connect to external resources\n",
    "Here we instantiate connections to the DICES database and to the Perseus Digital Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d02c2-a115-41f8-b758-b54d6f27295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DICES connection\n",
    "api = DicesAPI(logfile=\"dices.log\", logdetail=0)\n",
    "\n",
    "# create a new CTS connection\n",
    "cts = CtsAPI(dices_api=api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2d000-9269-49df-ada1-2a180f8e1a53",
   "metadata": {},
   "source": [
    "## 2. Download speeches\n",
    "\n",
    "It's a little hard to request these exact speeches from DICES. I'm going to:\n",
    "\n",
    "- request all speeches by Calypso to Odysseus\n",
    "- request all speeches by Dido to Aeneas\n",
    "- add the results together\n",
    "- look at them manually and note the IDs of the speeches I want\n",
    "- filter the results for just those IDs\n",
    "\n",
    "### Get speeches from DICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fa700-20d6-4f43-8866-a530a113b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download metadata on speeches by women in the Odyssey and the Aeneid\n",
    "speeches = (\n",
    "    api.getSpeeches(spkr_name=\"Calypso\", addr_name=\"Odysseus\") +\n",
    "    api.getSpeeches(spkr_name=\"Dido\", addr_name=\"Aeneas\")\n",
    ").sorted()\n",
    "\n",
    "# check that it worked\n",
    "print(len(speeches), \"speeches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4a792-f12e-42f5-adf1-4575ee499e57",
   "metadata": {},
   "source": [
    "### Make a speech table\n",
    "\n",
    "Here we create a Pandas data frame from our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d05ae-b610-42b3-a7ec-069b5bcb40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty list of rows\n",
    "rows = []\n",
    "\n",
    "# iterate over all speeches\n",
    "for speech in speeches:\n",
    "    \n",
    "    # create a new row\n",
    "    row = {\n",
    "        \"speech_id\": speech.id,\n",
    "        \"language\": speech.lang,\n",
    "        \"author\": speech.author.name,\n",
    "        \"work\": speech.work.title,\n",
    "        \"first_line\": speech.l_fi,\n",
    "        \"last_line\": speech.l_la,\n",
    "        \"speaker\": speech.getSpkrString(),\n",
    "        \"address\": speech.getAddrString(),\n",
    "    }\n",
    "\n",
    "    # add row to the list\n",
    "    rows.append(row)\n",
    "\n",
    "# make a table\n",
    "speech_table = pd.DataFrame(rows)\n",
    "\n",
    "# display\n",
    "display(speech_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe306bd-dddc-4750-ba13-a5043663cd49",
   "metadata": {},
   "source": [
    "### Filter on ID\n",
    "\n",
    "From the table, I can see that the speeches I want have IDs 820 and 1610. I'm going to create a new group with just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841d672-34fd-456b-9428-a6cddefe1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on ID\n",
    "farewells = speeches.filterIDs([820, 1610])\n",
    "\n",
    "# check that it worked\n",
    "for speech in farewells:\n",
    "    print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31a8ca-292c-4c96-8ece-90cc8525d115",
   "metadata": {},
   "source": [
    "### Download the text from Perseus\n",
    "\n",
    "Next, I'm going to download the text of both speeches from Perseus using the CTS connection I created earlier. The `getPassage()` method takes a DICES speech as its argument and returns the corresponding passage from Perseus. I'll save the passage as a new attribute of the the speech object to make sure that the speech and its text stay together.\n",
    "\n",
    "It's important to check that this step works... a few speeches in Perseus don't work well with CTS yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11040c7-fdc4-4f3e-91e1-45e69959e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over two farewell speeches\n",
    "for speech in farewells:\n",
    "    # download the text from perseus\n",
    "    speech.passage = cts.getPassage(speech)\n",
    "\n",
    "    # display the results\n",
    "    print(speech, \"\\n\")\n",
    "    print(speech.passage.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a8980-5127-4339-990e-ae0a348c977b",
   "metadata": {},
   "source": [
    "### Parse the text using natural language processing\n",
    "\n",
    "In this step, we process the downloaded text of the speeches using NLP. The method `runSpacyPipeline()` runs the appropriate spaCy pipeline based on the passage's language and saves the output to a new attribute called `spacy_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd419b-787b-493d-acfb-7a7964a1bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over two farewell speeches\n",
    "for speech in farewells:\n",
    "    # run spacy\n",
    "    speech.passage.runSpacyPipeline()\n",
    "\n",
    "    # check that it worked\n",
    "    print(speech, len(speech.passage.spacy_doc), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bb022-193c-4972-b2b9-8792f73a10d6",
   "metadata": {},
   "source": [
    "### Create a table of tokens\n",
    "\n",
    "This code creates a table with one row per token. Using nested `for` loops, we take each speech in turn, then each token of that speech. We build a record for the row that incorporates information about the speech and the token. At the end, we turn the list of rows into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6240a5-9cac-4c89-9d9b-84f8c6da54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty list of rows\n",
    "rows = []\n",
    "\n",
    "# iterate over speeches\n",
    "for speech in farewells:\n",
    "    \n",
    "    # iterate over tokens\n",
    "    for token in speech.passage.spacy_doc:\n",
    "\n",
    "        # create a record for this token\n",
    "        row = {\n",
    "            \"speech_id\": speech.id,\n",
    "            \"language\": speech.lang,\n",
    "            \"author\": speech.author.name,\n",
    "            \"title\": speech.work.title,\n",
    "            \"book\": speech.getPrefix(),                       # see note 1 below\n",
    "            \"line\": speech.passage.getLine(token)[\"n\"],       # see note 2\n",
    "            \"speaker\": speech.getSpkrString(),\n",
    "            \"addressee\": speech.getAddrString(),\n",
    "            \"token\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\": token.pos_,\n",
    "            \"verbform\": ''.join(token.morph.get(\"VerbForm\")), # see note 3\n",
    "            \"mood\": ''.join(token.morph.get(\"Mood\")),\n",
    "            \"voice\": ''.join(token.morph.get(\"Voice\")),\n",
    "            \"tense\": ''.join(token.morph.get(\"Tense\")),\n",
    "            \"person\": ''.join(token.morph.get(\"Person\")),\n",
    "            \"case\": ''.join(token.morph.get(\"Case\")),\n",
    "            \"gender\": ''.join(token.morph.get(\"Gender\")),\n",
    "            \"number\": ''.join(token.morph.get(\"Number\")),    \n",
    "        }\n",
    "\n",
    "        # add row to the list of rows\n",
    "        rows.append(row)\n",
    "\n",
    "# convert to a table\n",
    "tokens = pd.DataFrame(rows)\n",
    "\n",
    "# write the tokens to a CSV file\n",
    "tokens.to_csv(\"tokens.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# display the table\n",
    "display(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd09f60-0bc7-40fc-8ab6-26ef296f32c6",
   "metadata": {},
   "source": [
    "## 3. Word counts\n",
    "\n",
    "Beginning from the token table, we can create counts of any feature using `groupby()` and `agg()`.\n",
    "\n",
    "### Basic counts\n",
    "\n",
    "#### How many words does each character speak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940da6b4-6434-47b0-8458-ce653005d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count by character\n",
    "wc = (\n",
    "    tokens                            # original data \n",
    "    .groupby(\"speaker\")               # group by character\n",
    "    .agg(                             # define new table\n",
    "        tokens = (\"token\", \"count\"),  #   - token count per character\n",
    "    )\n",
    ")\n",
    "\n",
    "# show results\n",
    "display(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9b62d-4a5e-48e0-863a-450e9cc39131",
   "metadata": {},
   "source": [
    "#### Type-token ratio\n",
    "\n",
    "We can add to this the number of unique lemmata used by each character. The ratio of lemmata to tokens is one measure of language richness. Because Dido and Calypso speak different languages, and because their speeches are of different lengths, we might be cautious about drawing conclusions from this without further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d580f2-2b16-4788-a2df-25add7ea7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count by character\n",
    "wc = (\n",
    "    tokens                             # original data \n",
    "    .groupby(\"speaker\")                # group by character\n",
    "    .agg(                              # define new table\n",
    "        tokens = (\"token\", \"count\"),   #   - token count per character\n",
    "        lemmas = (\"lemma\", \"nunique\"), #   - number of unique lemmata\n",
    "    )\n",
    ")\n",
    "\n",
    "# add a column for the ratio of lemmata to tokens\n",
    "wc[\"ratio\"] = (wc[\"lemmas\"] / wc[\"tokens\"]).round(2)\n",
    "\n",
    "# show results\n",
    "display(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe86b0-1886-4c3d-81ff-c0a9a571a52c",
   "metadata": {},
   "source": [
    "### Ranked word list\n",
    "\n",
    "Let's create a table of word counts for each lemma in Calypso's speech. To examine her speech separately we can use `query()` to filter on speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3fbf1-bbba-4c83-a1e9-f93918f965c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word count by lemma\n",
    "wc = (\n",
    "    tokens                                 # original data\n",
    "    .query(f\"speaker=='Calypso'\")          # filter by speaker\n",
    "    .groupby(\"lemma\")                      # group by lemma\n",
    "    .agg(                                  # define new table\n",
    "        count = (\"token\", \"count\"),        #  - count how many tokens have this lemma\n",
    "    )\n",
    "    .sort_values(\"count\", ascending=False) # sort results from highest to lowest\n",
    ")\n",
    "\n",
    "# show results\n",
    "display(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9184f-f0a9-445b-9c19-b0e988e5e9a1",
   "metadata": {},
   "source": [
    "### Comparing speakers\n",
    "\n",
    "If I want to make lists for both speakers, I can use a `for` loop to avoid writing the same code twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664d7b5-dfcf-4808-8654-ba0a5c05a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider each speaker in turn\n",
    "for name in [\"Calypso\", \"Dido\"]:\n",
    "\n",
    "    # create a word count by lemma\n",
    "    wc = (\n",
    "        tokens                                 # original data\n",
    "        .query(f\"speaker=='{name}'\")           # filter by speaker\n",
    "        .groupby(\"lemma\")                      # group by lemma\n",
    "        .agg(                                  # define new table\n",
    "            count = (\"token\", \"count\"),        #  - count how many tokens have this lemma\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False) # sort results from highest to lowest\n",
    "    )\n",
    "\n",
    "    # show just the top of the list\n",
    "    display(name, wc.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7c14a-856d-4ce1-bb57-c94d749412fa",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "- the variable `name` is set to \"Calypso\" and \"Dido\" in turn\n",
    "- in the expression `f\"speaker=='{name}'\"`, the `f` before the quotation marks allows me to insert variables into the quoted text by using curly braces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98797d-a69f-4980-a787-39d08b129049",
   "metadata": {},
   "source": [
    "### Add frequency\n",
    "\n",
    "Because the two speeches are different lengths, it makes sense to divide by the total number of tokens. Word frequencies are often expressed per 1000 words, to make them easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45284084-582e-4f8f-9b45-54a4176896b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider each speaker in turn\n",
    "for name in [\"Calypso\", \"Dido\"]:\n",
    "\n",
    "    # create a word count by lemma\n",
    "    wc = (\n",
    "        tokens                                 # original data\n",
    "        .query(f\"speaker=='{name}'\")           # filter by speaker\n",
    "        .groupby(\"lemma\")                      # group by lemma\n",
    "        .agg(                                  # define new table\n",
    "            count = (\"token\", \"count\"),        #  - count how many tokens have this lemma\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False) # sort results from highest to lowest\n",
    "    )\n",
    "    \n",
    "    # calculate total tokens\n",
    "    total = wc[\"count\"].sum()\n",
    "    \n",
    "    # add frequency column\n",
    "    wc[\"freq\"] = (1000 * wc[\"count\"] / total).round(1)\n",
    "    \n",
    "    # show just the top\n",
    "    display(name, wc.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdf025-c4da-4b35-949e-3d2546e2a29a",
   "metadata": {},
   "source": [
    "### A miniature concordance\n",
    "\n",
    "Let's make use of some additional information in the table. Alongside the token count for each lemma, we can include a list of the unique forms that actually occur, plus a list of lines on which they are found.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Not all aggregation functions produce a single number. `unique` produces a list of values with duplicates removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6daa3c1-2704-4c84-8090-2114dc23a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider each speaker in turn\n",
    "for name in [\"Calypso\", \"Dido\"]:\n",
    "\n",
    "    # create a word count by lemma\n",
    "    wc = (\n",
    "        tokens                                 # original data\n",
    "        .query(f\"speaker=='{name}'\")           # filter by speaker\n",
    "        .groupby(\"lemma\")                      # group by lemma\n",
    "        .agg(                                  # define new table\n",
    "            count = (\"token\", \"count\"),        #  - count how many tokens have this lemma\n",
    "            forms = (\"token\", \"unique\"),       #  - which forms are being counted\n",
    "            lines = (\"line\", \"unique\"),        #  - which lines does it occur on\n",
    "        )\n",
    "        .sort_values(\"count\", ascending=False) # sort results from highest to lowest\n",
    "    )\n",
    "    \n",
    "    # calculate total tokens\n",
    "    total = wc[\"count\"].sum()\n",
    "    \n",
    "    # add frequency column\n",
    "    wc.insert(1, \"freq\", (1000 * wc[\"count\"] / total).round(1))\n",
    "    \n",
    "    # show just the top\n",
    "    display(name, wc.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
